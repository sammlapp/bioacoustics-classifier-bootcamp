# Session 4 Lecture Outline — Embeddings & Transfer Learning
*(Based on transcript starting at 15:10:54)*
*transcript and summary both auto-generated by machines*


## 15:14 — Introducing Transfer Learning
- What to do with **very small datasets** (e.g., only 10 samples).
- Motivation for **transfer learning**.
- Overview of workflow:
  - Input (spectrogram as numbers)
  - Architecture → parameters → output predictions
- Concept of **splitting model** into:
  - **Feature extractor**
  - **Classifier**

---

## 15:15 — Feature Extractors & Classifiers
- Classifier = last layer converting features into class scores.
- Feature extractor = all layers before the classifier.
- Embedding = the numeric representation before classification.
- Key terms:
  - **Feature:** set of numbers describing the input.
  - **Embedding:** same as feature vector (from deep network).

---

## 15:18 — Handcrafted vs. Deep Features
- **Handcrafted features:**
  - Derived from known properties (e.g., F₀, duration, MFCCs).
- **Deep features:**
  - Derived automatically from trained networks.

---

## 15:19 — Traditional vs. Deep Learning
- Traditional ML: manually extract features → train simple classifier.
- Deep learning: model learns features + classifier jointly.
- Embeddings separate these again for reuse.

---

## 15:21 — Discussion: Semantic Meaning of Features
- Question: Do deep features correspond to human-interpretable ones (like F₀ or bandwidth)?
  - Usually **no**, information is distributed across many features.

---

## 15:23 — Embedding Layer Behavior
- Question: Are embedding values deterministic?
  - Yes, once trained (inference mode = deterministic).
  - Only stochastic during training.

---

## 15:25 — Class-Specific Weights
- Each class uses same embedding features but different final weights.
- Only the final layer’s weights are class-specific.

---

## 15:26 — Scaling to Many Classes
- Question: What if more classes than features?
  - Works surprisingly well (e.g., Perch trained on 15k species).
- Insight: Embeddings generalize across classes.

---

## 15:27 — Transfer Learning Explained
- If trained on many species:
  - The same embeddings work for unseen species.
  - Only need new classifier weights.
- **Frozen embeddings** = use pre-trained model features.
- **Shallow classifier** = new small model on top.

---

## 15:29 — Overfitting and Flexibility
- Many parameters → risk of **overfitting**.
- Shallow classifiers (few parameters) → more robust.
- Linear classifier = fitting a **plane** separating classes.

---

## 15:30 — Model Flexibility
- Flexibility determined by **architecture**, not hyperparameters.
- Deep models = high flexibility, high overfitting risk.
- Trade-off between:
  - Too rigid (underfit)
  - Too flexible (overfit)

---

## 15:32 — Adding Flexibility
- Add **non-linear layers** (hidden layers) for more flexibility.
- Neural networks = stacked weighted sums + activations.
- Introduce **two-layer MLP (Multi-Layer Perceptron)**.

---

## 15:35 — Neural Network Structure
- Inputs → hidden layer → output layer.
- Hidden layer size = number of “nodes”.
- Example: 1,000 inputs × 100 hidden nodes = 100k weights.
- Trade-off: flexibility vs. overfitting.

---

## 15:37 — Hyperparameters & Optimization
- Hidden layer size and number of layers = **hyperparameters**.
- Too many → overfitting.
- Check **training vs evaluation** performance:
  - High train / low eval = overfitting.

---

## 15:41 — Regularization
- Reduces overfitting by penalizing large weights.
- Implemented via **weight decay** in optimizers (SGD, Adam).
- Common in PyTorch training loops.

---

## 15:42 — Activation Functions
- Map arbitrary outputs to [0,1].
- **Sigmoid**: for multi-label (independent class predictions).
- **Softmax**: for single-label (mutually exclusive classes).

---

## 15:46 — Class Dependence Discussion
- Idea: classes may not be independent (e.g., similar songs).
- In practice: model already captures correlations through shared features.
- Active learning loops can incorporate human feedback.

---

## 15:49 — Beyond Transfer Learning
- Other uses of embeddings:
  - **Visualization**
  - **Clustering**
  - **Similarity search**
  - **Individual identification**

---

## 15:53 — Dimensionality Reduction
- Compress feature vectors (e.g., 1024 → 2D) for visualization.
- Techniques:
  - **t-SNE**, **UMAP** (non-linear)
  - **PCA** (linear)
- Use for clustering, visualization, or similarity search.

---

## 15:58 — Other Classifiers
- Beyond MLP:
  - Random Forests
  - Decision Trees
  - SVMs, etc.
- Worth testing different classifiers for small datasets.

---

## 15:59 — Choosing Feature Extractors
- Use **existing** feature extractors if:
  - Limited training data.
  - Similar domain to your target task.
- Train your own if:
  - You have abundant labeled data.
  - Your sound type is underrepresented.

---

## 16:01 — Fine-Tuning
- Strategy: start from pre-trained model and **update weights slightly**.
- Small learning rate on feature extractor → “fine-tuning”.
- Requires thousands of labeled samples.
- Risk: overfitting if dataset is small.

---

## 16:04 — Framework Support
- Some models can’t be fine-tuned (compiled).
- Examples:
  - **BirdNet**: not fine-tunable.
  - **Perch**: technically open, but TensorFlow-based.
  - **HawkEars**, **BirdSet**: fine-tuning supported in PyTorch.

---

## 16:06 — How Much Data for Fine-Tuning?
- Recommended: ≥1,000 labeled samples.
- Below that, use **frozen embeddings + shallow classifier**.

---

## 16:07 — Implementation in OpenSoundscape
- Steps:
  1. Load pre-trained model from **Model Zoo**.
  2. Generate embeddings with `.embed()`.
  3. Train **MLPClassifier** using embeddings.
  4. Adjust hidden layer size and steps.
- Training is fast (10s–minutes).

---

## 16:10 — Assignments & Next Steps
- **This week:**
  - Create CNN training script.
  - Try small-scale training.
  - Push training script to GitHub.
- **Next week:** Model evaluation.

---

## ✅ Key Takeaways
- Embeddings are numeric feature representations of inputs.
- Feature extractors from pre-trained models can generalize widely.
- Shallow classifiers reduce overfitting risk for small datasets.
- Fine-tuning enables adaptation but requires substantial data.
- Visualization and similarity search reveal structure in embedding space.
