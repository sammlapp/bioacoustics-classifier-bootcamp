# Machine Learning for Bioacoustics - Session 3 Transcript Summary

## Check-in & Progress Updates (15:04:40)
- Students working on Python notebooks from last week
- Progress on loading audio clips as OpenSoundscape objects
- Annotation work ongoing - challenges with multiple overlapping birds (8 at once)
- Strategy: Set aside particularly difficult files, focus on quantity
- Python environment setup in progress on lab laptops

## Project Organization (15:06:35)

### The Problem: Disorganization (15:07:10)
- Instructor showed screenshot of messy project folder
- Issues: old versions mixed with new, unclear naming conventions
- Data, scripts, outputs, and figures all mixed together
- Experimentation phase allows messiness but structure needed eventually

### Key Principles for Organization (15:08:09)

#### 1. Separate into Folders (15:08:35)
Four main categories:
- **Implementation code**: Functions, classes, methods used across analyses
- **Analysis code**: Scripts and notebooks that process data (inspecting, training, evaluating)
- **Data**: Input files (audio, metadata)
- **Outputs**: Results, figures, model checkpoints

Example structure shown from GitHub project with 6 folders

#### 2. Document (15:11:00)
Three types of documentation:

**README.md files** (15:11:06)
- Top level of project folder
- Describes purpose and what's in repository
- Lists and explains each folder/file

**Docstrings** (15:11:50)
- Triple quotes after function/class definitions
- Describes what function does and lists arguments
- Can be auto-generated by AI tools
- Accessible via Python help system

**Comments** (15:13:34)
- Use # symbol
- Should explain purpose and reasoning, not just restate code
- Place before code blocks

**Personal notes document** (15:14:07)
- Keep in Google Docs outside Git repository
- Track paths, scripts, observations, next steps
- Helpful when returning to project after time away

#### 3. Enumerate Files (15:15:10)
- Number files logically (1, 2, 3, 4, 5)
- Follows workflow: load annotations → visualize → train → evaluate
- Use 1A, 1B if need to insert steps
- Simple organizational hack that's very effective

#### 4. Use Best Practices for Reproducibility (15:16:14)

**Environment YAML files** (15:16:32)
- Lists all packages and exact versions (e.g., 1.0.2)
- Created with `conda env export`
- Essential for running code on different machines or at different times
- Can maintain multiple versions for different scripts

**Git versioning and tags** (15:17:51)
- Keep single working copy, use Git history for old versions
- **Tags/Releases**: Named versions at specific points (e.g., "Version 1", "Version October 15th")
- Can always return to tagged versions
- Write meaningful commit messages to track changes between releases
- Example shown: bioacoustics-model-zoo versions (0.11.0, 0.12.0)
- Tags align with environment files for full reproducibility

### Discussion Points (15:22:42)
- File numbering very useful - makes copying entire processes easier
- Numbering helps identify what order files run in
- Not always clear at project start what final structure will be
- ML projects usually have predictable overall structure

## Data Splitting (15:24:18)

### Introduction to Classification Problem (15:25:04)
- Goal: Assign class(es) to samples (images, spectrograms)
- Formulation: Determine class Y given data X
- Example: 2D points classified as orange or blue using linear boundary

### Why Hold Out Data? (15:27:05)
- Can't know if model works on new data if we use all data for training
- Solution: Hold out subset during training, evaluate later
- Important because complex models can create convoluted decision boundaries (overfitting)
- Different models may all perfectly classify training data but perform differently on held-out data
- Allows **model selection**: choosing which model is actually better

### Train, Validation, and Test Sets (15:29:49)

**Training set**: Data used to fit the model

**Validation set** (15:29:32):
- Held out from training
- Used for model selection
- Prevents overfitting to training data

**Test set** (15:29:56):
- Why needed beyond validation?
- When testing 100+ models on validation set, might accidentally find one that does well by chance
- Test set: Final performance measure after selecting best model(s)
- Used only after model selection complete
- Gives unbiased performance estimate

### Domain Shift Problem (15:31:18)

**The Challenge** (15:31:46):
- Training data often not representative of application data
- Examples: focal recordings vs field recordings, one site vs multiple sites, peak season vs full season
- Model may perform well on validation set but poorly on real application data
- Properties differ between training and application scenarios

**Key Strategy** (15:33:08):
- Training/validation: Can use larger volume of easier-to-obtain data (focal recordings, subset of sites)
- **Test set should be representative of actual application scenario**
- Even with non-representative training data, representative test set shows which models generalize best
- Counterintuitive: Best use of high-quality representative data is often in test set, not training set

### Domain Shift in Audio Data (15:34:45)

**Training data characteristics**:
- Often clean focal recordings from online libraries (Xeno-Canto, Macaulay)
- Loud, clean, lack overlapping sounds

**Application data characteristics**:
- Field recordings
- Background noise, quieter target sounds
- Overlapping vocalizations

**Example**: Training only on clean examples, then testing with background bird - easy to miss the target sound

### When to Use Different Splits (15:36:28)

**Ideal scenario** (15:36:33):
- Random subset of field recordings for all data (train, validation, test)
- Only possible when: species not rare, dataset manageable size, sufficient annotation effort

**Realistic scenarios** (15:36:56):
- Rare species
- Huge field recording sets
- Applying to future years of data not yet collected
- Usually must select specific recordings to label

**Discussion example** (15:37:21):
- Student using field recordings plus classifier outputs to find training data
- Not truly random, but smaller domain shift than using Macaulay recordings

### Combining Data Types (15:38:36)
**Question**: Would combining focal and field recordings hurt performance?

**Answer**: 
- Usually more data wins
- More examples + more diversity = better performance
- Can use data augmentation to make focal recordings resemble field data
- Add noise, reduce volume, overlay other sounds
- Must be careful not to make quiet sounds disappear completely

### Data Leakage (15:41:06)

**Definition**: When evaluation sets have unintentional similarity to training set

**Philosophy**: Test set should be separated so good performance means model learned something generalizable, not memorized

**Common problem in audio** (15:41:42):
- Splitting clips from same audio file across train/validation/test
- Clips 3 seconds apart may have same individuals, same vocalizations
- **Temporal autocorrelation**: What you hear seconds/minutes later is similar
- Model can memorize specific file features and still do well on validation
- Not a strong evaluation

**Critical rule** (15:42:49):
**Never split clips from same audio file across data splits**
- Keep entire files together in one split (all train, all validation, or all test)
- Then create as many clips from those files as needed

**Spatial autocorrelation** (15:44:09):
- Same issue with recordings from same location/recorder
- Same individuals may appear repeatedly
- Model might only recognize specific individual, not generalize

### Levels of Data Splitting (15:44:32)

**Increasing difficulty**:
1. **By clip** (easiest, don't do this): Random split of all clips - causes data leakage
2. **By file**: Entire files go into one split each
3. **By recorder/location**: All recordings from each recorder stay together
4. **By region**: Entire state forests or geographic areas held out

**Which level to use?** (15:45:27)
- Depends on what you want to learn about your model
- Depends on scope of model's application
- Split at level where you need model to generalize

### Example Scenarios (15:46:16)

**Scenario 1** (15:46:26):
- Have labeled data from 6 state parks
- Want to apply to 20 state parks (including unlabeled ones)
- **Strategy**: Hold out entire parks (train on 4, evaluate on 2)
- Tests whether model generalizes to completely new parks

**Scenario 2** (15:46:53):
- Have labeled data from 6 state parks
- Will only apply model to these same 6 parks
- **Strategy**: Include all 6 parks in training and evaluation (split by file/recorder within parks)
- Most representative evaluation covers all parks where you'll apply model
- No need to hold out entire parks

**Note**: These aren't mutually exclusive - can do both experiments

### General Data Splitting Strategy (15:49:53)

1. **If you have annotations from random sample of application**: Use random split for test set
2. **If you can't create random annotations**: Get as close as possible (e.g., check classifier detections)
3. **Remaining labeled audio**: Divide randomly into training and validation (still split by file, not clip)

### Thinking Exercise (15:50:47)
Students given time to consider for their own projects:
- What does field application look like?
- What annotated data is available?
- How to split data appropriately?

### Student Questions on Splitting (15:52:37)

**Xeno-Canto recordings** (15:52:37):
- Could split by recorder/recordist if suspected similarity
- Depends on data quantity available
- May not have representative field data yet

**Rare species with limited data** (15:54:24):
- Example: 100 total positive clips across Macaulay and field recordings
- Still valuable to hold out validation set even with small dataset
- Why? Model can perfectly memorize 100 training samples but fail on new data
- Validation set is only way to distinguish memorization from generalization
- **Value of validation data > value of adding those clips to training**
- Most valuable validation clips: those resembling field recordings

**Different repositories** (15:57:00):
- Question: Use Macaulay for training, Xeno-Canto for test?
- Problem: This tests generalization between repositories, not to field conditions
- Generalization is not a general property - models generalize to specific types of shifts
- Need to match evaluation to actual application domain

**Very rare species** (15:58:47):
- May not find any examples in field audio until after making classifier
- Strategy: Make validation set from available data (even if not from field)
- At least shows you're not just memorizing training samples
- Apply model, evaluate by inspecting scores at different confidence levels
- Accept that you won't know recall without representative test set
- As you find detections, build test set iteratively
- Can't make assumptions about recall at specific thresholds without field test data

## Class Balance (16:01:00)

### How Much Data in Each Split? (16:01:05)
**Common splits**:
- Training/Validation only: 80-20
- Train/Val/Test: 80-10-10, 60-20-20, or 70-15-15
- Bulk should be training (over 50%)

**K-Fold Cross-Validation** (16:02:00):
- Useful when dataset very small (e.g., 20 samples)
- 80-20 split = train on 16, evaluate on 4 (too few)
- Solution: Multiple splits, evaluate on different subsets of 4
- Get more robust performance estimate
- Common in shallow ML (decision trees, logistic regression)
- Less common in deep learning (takes longer to train, usually have more data)

### Training Set Balance (16:03:21)

**What the model learns** (16:03:41):
Model always learns the ratio of positives to negatives in training data
- Even if learning nothing else useful, learns class frequencies

**Problems with imbalanced data** (16:04:18):
Example: 10 positives, 1000 negatives
- Model learns to predict zero on every sample
- Gets 99.9% accuracy by saying everything is negative
- Not useful, but high score
- Numerical issue: weights become zeros and can't recover

**Solution 1: Upsampling** (16:05:23):
- Don't throw away negative samples (each adds value through diversity)
- Replicate positive samples to match negative count
- Example: 10 positives replicated 100 times = 1000 "positives" + 1000 negatives
- Model equally likely to see positive or negative during training
- Prevents taking easy path of predicting all zeros

**Solution 2: Weighted Loss Function** (16:06:31):
- Adjust loss function to heavily penalize false negatives
- Model can't just predict everything as zero

**Why upsampling works** (16:06:55):
- Seems like would overfit on 10 positives
- But data augmentation helps: Each replicated sample augmented differently
- Especially overlay/mixup augmentation mixes with different audio
- More like 100 different samples than one repeated sample

### Upsampling Explained (16:13:11)

**Visualization on board** (16:13:22):

Scenario A: 10 positives, 10 negatives, 100 epochs
- See each of 20 samples 100 times total

Scenario B: 10 positives (replicated 100x), 1000 negatives, 1 epoch  
- See each positive 100 times
- See 1000 different negatives once each

**Key insight**: Either way you see positives 100 times
- With upsampling: Get 1000 diverse negatives instead of 10 repeated negatives
- No downside to keeping many negatives
- Increasing negative diversity helps model differentiate better

### Evaluation Set Balance (16:07:47)

**Considerations**:
- Need enough positive samples for meaningful evaluation
- 1 positive + 100 negatives = no information on model quality

**Representative balance** (16:08:44):
- Ideally evaluation set reflects application data class balance
- Affects metrics like precision and average precision
- These metrics change with different class balances

**If using balanced evaluation set** (16:09:27):
- Fine to use 50-50 even if field data is 1% positive
- Remember: Some metrics sensitive to class balance (precision, average precision)
- Some metrics not sensitive: Area under precision-recall curve
- Same classifier will have different precision scores with different balances

### Practical Questions (16:10:15)

**How many negatives to collect?** (16:10:47)
- More is better - real value in 10,000 vs 1,000 vs 500 negatives
- Model performs better with more unique negatives
- Can randomly sample field data for negatives
- Can overlay negatives with positives as augmentation
- Then upsample positives to match

**Example strategy** (16:11:41):
- 10,000 negatives from random field sampling
- 60 positives
- Upsample positives to create balanced training set
- Use augmentation (especially overlay) so each positive looks different each time

## Wrap-up and Next Steps (16:16:11)

### Homework for Next Week (16:16:15)
- Finish set of annotated recordings (doesn't need to be complete final set)
- Ingest annotations into Python notebook
- Tutorial provided for loading Raven or Audacity annotations
- Next session: Machine learning classification (diving in!)
- Will cover clip length and other details
- Need data ready to start training classifier

**Topics for next week**:
- Machine learning classification
- Clip length decisions
- Generating clips from annotations (tutorial already linked)

### Final Questions (16:18:03)
- Student needs refresher on generating clips from Raven annotations
- Instructor confirms that's the pre-class assignment tutorial
- Session ends

---

## Key Takeaways

1. **Organization**: Separate code/data/outputs, document thoroughly, enumerate files logically, use version control
2. **Data Splitting**: Never split clips from same file across train/val/test; split level depends on application scope
3. **Domain Shift**: Representative test set more important than representative training set when can't have both
4. **Class Balance**: Upsample positives in training to match negatives; use augmentation to increase diversity
5. **Evaluation**: Need enough samples for meaningful metrics; some metrics sensitive to class balance
6. **Reproducibility**: Environment YAML files and Git tags essential for returning to previous states
